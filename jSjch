@patch('main.storage.Client')
def test_upload_timeseries_df(mock_storage_client):
    # Setup sample DataFrames for the inputs
    product_info_df = pd.DataFrame({
       'PRD_CD': [1, 2, 3],
       'missing_ratio': [0.1, 0.3, 0.05],
       'term': [200, 100, 220],
       'missing_ratio_13': [0.4, 0.6, 0.2],
       'URISU_AVE13WK': [3, 1, 5]
    })
    unique_tenpo_df = pd.DataFrame({
       'PRD_CD': [1, 2, 3, 4],
       'TENPO_CD': [760, 760, 760, 760],
       'cls_cd': [10, 20, 10, 20],
       'line_cd': [100, 200, 100, 200],
       'hnmk_cd': [1000, 2000, 1000, 2000],
       'baika_toitsu': [1, 2, 1, 2],
       'low_price_kbn': [0, 1, 0, 1],
       'nenshudo': [202501, 202502, 202503, 202504],
       'URI_SU': [10, 20, 30, 40],
       'URI_KIN': [100, 200, 300, 400],
       'BAIKA': [1000, 2000, 3000, 4000],
       'prd_nm_kj': ['prod1', 'prod2', 'prod3', 'prod4'],
       'sell_start_ymd': [20200101, 20200201, 20200301, 20200401]
    })
    tenpo_df = pd.DataFrame({
       'TENPO_CD': [760],
       'CHUSHA_KANO_DAISU': [5],
       'OKUNAI_URIBA_MENSEKI': [50],
       'OKUGAI_URIBA_MENSEKI': [25],
       'SHIKICHI_MENSEKI': [20]
    })
    dpt = 69
    tenpo = 760
    threshold_missing_ratio = 0.2
    threshold_timeseries_length = 180
    path_upload_tmp_local = "/tmp/test_timeseries.csv"
    path_upload_time_series_blob = "path/to/blob/{}_{}.csv"
    path_upload_time_series_blob2 = "path/to/blob2/{}_{}.csv"
    bucket_name = "dummy_bucket"
    output_6wk_2sales = True
    # Mock blob upload
    mock_bucket = MagicMock()
    mock_blob = MagicMock()
    mock_bucket.blob.return_value = mock_blob
    mock_storage_client.return_value.bucket.return_value = mock_bucket
    # Call the function
    time_series_prod_cd, time_series_df = short_term_preprocess_common.upload_timeseries_df(
       product_info_df,
       unique_tenpo_df,
       tenpo_df,
       dpt,
       tenpo,
       threshold_missing_ratio,
       threshold_timeseries_length,
       path_upload_tmp_local,
       path_upload_time_series_blob,
       path_upload_time_series_blob2,
       bucket_name,
       output_6wk_2sales
    )
    # Assertions on the output
    # The product codes filtered should respect threshold conditions
    assert all(product_info_df.loc[product_info_df['PRD_CD'].isin(time_series_prod_cd), 'missing_ratio'] <= threshold_missing_ratio)
    assert all(product_info_df.loc[product_info_df['PRD_CD'].isin(time_series_prod_cd), 'term'] >= threshold_timeseries_length)
    # The time_series_df columns should include expected columns
    expected_columns = [
       'PRD_CD', 'TENPO_CD', 'cls_cd', 'line_cd', 'hnmk_cd', 'baika_toitsu',
       'low_price_kbn', 'nenshudo', 'URI_SU','URI_KIN','BAIKA',
       'prd_nm_kj', 'sell_start_ymd',
       'CHUSHA_KANO_DAISU', 'OKUNAI_URIBA_MENSEKI', 'OKUGAI_URIBA_MENSEKI', 'SHIKICHI_MENSEKI'
    ]
    for col in expected_columns:
        assert col in time_series_df.columns
    # Validate that upload_from_filename was called twice (due to output_6wk_2sales=True)
    assert mock_blob.upload_from_filename.call_count == 2






___________________________________________________________ test_extract_as_df ___________________________________________________________

mock_storage_client = <MagicMock name='Client' id='140630370948944'>

    @patch('main.storage.Client')
    def test_extract_as_df(mock_storage_client):
        # Prepare a sample DataFrame to be returned from mocked CSV read
        expected_df = pd.DataFrame({
           'BUMON_CD': [84, 84, 84, 84, 84],
           'PRD_CD': [4549509192435, 4511413407363, 4511413404164, 4549509190035, 4511413404386],
           'TENPO_CD': [760, 760, 760, 760, 760],
           'NENSHUDO': [202530, 202530, 202530, 202530, 202530],
           'BAIKA': [98, 358, 398, 108, 438],
           'URI_SU': [1, 1, 1, 4, 1],
           'URI_KIN': [90, 332, 369, 400, 406]
        })
        csv_bytes = expected_df.to_csv(index=False).encode('utf-8')
        mock_bucket = MagicMock()
        mock_blob = MagicMock()
        # Blob open reads csv_bytes just as file would
        mock_blob.open.return_value.__enter__.return_value = csv_bytes.decode('utf-8').splitlines()
        mock_bucket.blob.return_value = mock_blob
        mock_storage_client.return_value.bucket.return_value = mock_bucket
        # Call the function under test
>       df = common.extract_as_df('dummy_path.csv', 'dev-cainz-demandforecast', encoding='utf-8', usecols=None)

test2/test_main.py:214: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
repos/cainz_demand_forecast/cainz/common/common.py:147: in _decorator_logging
    result = func(*args, **kwargs)
repos/cainz_demand_forecast/cainz/common/common.py:171: in extract_as_df
    df = pd.read_csv(f, encoding=encoding, usecols=usecols)
/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py:311: in wrapper
    return func(*args, **kwargs)
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:586: in read_csv
    return _read(filepath_or_buffer, kwds)
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:482: in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:811: in __init__
    self._engine = self._make_engine(self.engine)
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1040: in _make_engine
    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:51: in __init__
    self._open_handles(src, kwds)
/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py:222: in _open_handles
    self.handles = get_handle(
/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:609: in get_handle
    ioargs = _get_filepath_or_buffer(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

filepath_or_buffer = ['BUMON_CD,PRD_CD,TENPO_CD,NENSHUDO,BAIKA,URI_SU,URI_KIN', '84,4549509192435,760,202530,98,1,90', '84,4511413407363,76...,4511413404164,760,202530,398,1,369', '84,4549509190035,760,202530,108,4,400', '84,4511413404386,760,202530,438,1,406']
encoding = 'utf-8', compression = {'method': None}, mode = 'r', storage_options = None

    def _get_filepath_or_buffer(
        filepath_or_buffer: FilePathOrBuffer,
        encoding: str = "utf-8",
        compression: CompressionOptions = None,
        mode: str = "r",
        storage_options: StorageOptions = None,
    ) -> IOArgs:
        """
        If the filepath_or_buffer is a url, translate and return the buffer.
        Otherwise passthrough.
    
        Parameters
        ----------
        filepath_or_buffer : a url, filepath (str, py.path.local or pathlib.Path),
                             or buffer
        compression : {{'gzip', 'bz2', 'zip', 'xz', None}}, optional
        encoding : the encoding to use to decode bytes, default is 'utf-8'
        mode : str, optional
    
        storage_options : dict, optional
            Extra options that make sense for a particular storage connection, e.g.
            host, port, username, password, etc., if using a URL that will
            be parsed by ``fsspec``, e.g., starting "s3://", "gcs://". An error
            will be raised if providing this argument with a local path or
            a file-like buffer. See the fsspec and backend storage implementation
            docs for the set of allowed keys and values
    
            .. versionadded:: 1.2.0
    
        ..versionchange:: 1.2.0
    
          Returns the dataclass IOArgs.
        """
        filepath_or_buffer = stringify_path(filepath_or_buffer)
    
        # handle compression dict
        compression_method, compression = get_compression_method(compression)
        compression_method = infer_compression(filepath_or_buffer, compression_method)
    
        # GH21227 internal compression is not used for non-binary handles.
        if compression_method and hasattr(filepath_or_buffer, "write") and "b" not in mode:
            warnings.warn(
                "compression has no effect when passing a non-binary object as input.",
                RuntimeWarning,
                stacklevel=2,
            )
            compression_method = None
    
        compression = dict(compression, method=compression_method)
    
        # uniform encoding names
        if encoding is not None:
            encoding = encoding.replace("_", "-").lower()
    
        # bz2 and xz do not write the byte order mark for utf-16 and utf-32
        # print a warning when writing such files
        if (
            "w" in mode
            and compression_method in ["bz2", "xz"]
            and encoding in ["utf-16", "utf-32"]
        ):
            warnings.warn(
                f"{compression} will not write the byte order mark for {encoding}",
                UnicodeWarning,
            )
    
        # Use binary mode when converting path-like objects to file-like objects (fsspec)
        # except when text mode is explicitly requested. The original mode is returned if
        # fsspec is not used.
        fsspec_mode = mode
        if "t" not in fsspec_mode and "b" not in fsspec_mode:
            fsspec_mode += "b"
    
        if isinstance(filepath_or_buffer, str) and is_url(filepath_or_buffer):
            # TODO: fsspec can also handle HTTP via requests, but leaving this
            # unchanged. using fsspec appears to break the ability to infer if the
            # server responded with gzipped data
            storage_options = storage_options or {}
    
            # waiting until now for importing to match intended lazy logic of
            # urlopen function defined elsewhere in this module
            import urllib.request
    
            # assuming storage_options is to be interpreted as headers
            req_info = urllib.request.Request(filepath_or_buffer, headers=storage_options)
            with urlopen(req_info) as req:
                content_encoding = req.headers.get("Content-Encoding", None)
                if content_encoding == "gzip":
                    # Override compression based on Content-Encoding header
                    compression = {"method": "gzip"}
                reader = BytesIO(req.read())
            return IOArgs(
                filepath_or_buffer=reader,
                encoding=encoding,
                compression=compression,
                should_close=True,
                mode=fsspec_mode,
            )
    
        if is_fsspec_url(filepath_or_buffer):
            assert isinstance(
                filepath_or_buffer, str
            )  # just to appease mypy for this branch
            # two special-case s3-like protocols; these have special meaning in Hadoop,
            # but are equivalent to just "s3" from fsspec's point of view
            # cc #11071
            if filepath_or_buffer.startswith("s3a://"):
                filepath_or_buffer = filepath_or_buffer.replace("s3a://", "s3://")
            if filepath_or_buffer.startswith("s3n://"):
                filepath_or_buffer = filepath_or_buffer.replace("s3n://", "s3://")
            fsspec = import_optional_dependency("fsspec")
    
            # If botocore is installed we fallback to reading with anon=True
            # to allow reads from public buckets
            err_types_to_retry_with_anon: list[Any] = []
            try:
                import_optional_dependency("botocore")
                from botocore.exceptions import (
                    ClientError,
                    NoCredentialsError,
                )
    
                err_types_to_retry_with_anon = [
                    ClientError,
                    NoCredentialsError,
                    PermissionError,
                ]
            except ImportError:
                pass
    
            try:
                file_obj = fsspec.open(
                    filepath_or_buffer, mode=fsspec_mode, **(storage_options or {})
                ).open()
            # GH 34626 Reads from Public Buckets without Credentials needs anon=True
            except tuple(err_types_to_retry_with_anon):
                if storage_options is None:
                    storage_options = {"anon": True}
                else:
                    # don't mutate user input.
                    storage_options = dict(storage_options)
                    storage_options["anon"] = True
                file_obj = fsspec.open(
                    filepath_or_buffer, mode=fsspec_mode, **(storage_options or {})
                ).open()
    
            return IOArgs(
                filepath_or_buffer=file_obj,
                encoding=encoding,
                compression=compression,
                should_close=True,
                mode=fsspec_mode,
            )
        elif storage_options:
            raise ValueError(
                "storage_options passed with file object or non-fsspec file path"
            )
    
        if isinstance(filepath_or_buffer, (str, bytes, mmap.mmap)):
            return IOArgs(
                filepath_or_buffer=_expand_user(filepath_or_buffer),
                encoding=encoding,
                compression=compression,
                should_close=False,
                mode=mode,
            )
    
        if not is_file_like(filepath_or_buffer):
            msg = f"Invalid file path or buffer object type: {type(filepath_or_buffer)}"
>           raise ValueError(msg)
E           ValueError: Invalid file path or buffer object type: <class 'list'>

/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:396: ValueError
----------------------------------------------------------- Captured log call ------------------------------------------------------------
INFO     extract_as_df:common.py:143 start
ERROR    extract_as_df:common.py:150 Exception in extract_as_df: Invalid file path or buffer object type: <class 'list'>
Traceback (most recent call last):
  File "/home/jupyter/Refactored_Files/cloudrun/stage1_20250328_218newstr_upld_casepackbara/repos/cainz_demand_forecast/cainz/common/common.py", line 147, in _decorator_logging
    result = func(*args, **kwargs)
  File "/home/jupyter/Refactored_Files/cloudrun/stage1_20250328_218newstr_upld_casepackbara/repos/cainz_demand_forecast/cainz/common/common.py", line 171, in extract_as_df
    df = pd.read_csv(f, encoding=encoding, usecols=usecols)
  File "/opt/conda/lib/python3.10/site-packages/pandas/util/_decorators.py", line 311, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 586, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 482, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 811, in __init__
    self._engine = self._make_engine(self.engine)
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1040, in _make_engine
    return mapping[engine](self.f, **self.options)  # type: ignore[call-arg]
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py", line 51, in __init__
    self._open_handles(src, kwds)
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/base_parser.py", line 222, in _open_handles
    self.handles = get_handle(
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/common.py", line 609, in get_handle
    ioargs = _get_filepath_or_buffer(
  File "/opt/conda/lib/python3.10/site-packages/pandas/io/common.py", line 396, in _get_filepath_or_buffer
    raise ValueError(msg)
ValueError: Invalid file path or buffer object type: <class 'list'>
INFO     extract_as_df:common.py:155 end elapsed time: 0.0035 seconds
============================================================ warnings summary ============================================================
test2/test_main.py::test_get_chanceloss_data
  /home/jupyter/Refactored_Files/cloudrun/stage1_20250328_218newstr_upld_casepackbara/main.py:912: SettingWithCopyWarning: 
  A value is trying to be set on a copy of a slice from a DataFrame
  
  See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
    df_target['CHANCE_LOSS_KN'][df_target['CHANCE_LOSS_PRD_SU'].astype(int) == 0] = 0.0

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
======================================================== short test summary info =========================================================
FAILED test2/test_main.py::test_extract_as_df - ValueError: Invalid file path or buffer object type: <class 'list'>
================================================= 1 failed, 5 passed, 1 warning in 1.73s =================================================
(base) jupyter@tcs-short-term-1:~/Refactored_Files/cloudrun/stage1_20250328_218newstr_upld_casepackbara$ 
