import pytest
import pandas as pd
import numpy as np
from unittest.mock import patch, MagicMock
from io import BytesIO
import datetime
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from main import (
    odas_correct,
   calc_nenshudo, calc_nenshudo2,
   load_sales_data, process_minmax_data,
   restrict_tenpo_hacchu_end_func
)

from repos.cainz_demand_forecast.cainz.common import common

# For demonstration, assuming functions are directly accessible here.
# Dummy CSV bytes helper
def make_csv_bytes(df):
    buffer = BytesIO()
    df.to_csv(buffer, index=False)
    buffer.seek(0)
    return buffer.read()

@pytest.fixture
def dummy_sales_df():
    return pd.DataFrame({
       "nenshudo": [202501, 202502, 202503],
       "PRD_CD": [123, 123, 123],
       "URI_SU": [10, np.nan, 20],
       "TENPO_CD": [1, 1, 1],
       "baika_toitsu": [0, 2, np.nan],
       "BAIKA": [100, 101, 102],
       "DPT": [5, 5, 5],
       "line_cd": [10, 10, 10],
       "cls_cd": [1, 1, 1],
       "hnmk_cd": [1000, 1000, 1000]
   })


@pytest.fixture
def dummy_calendar_df():
    return pd.DataFrame({
       "nenshudo": [202501, 202502, 202503],
       "shudo": [1,2,3],
       "week_from_ymd": [20250101, 20250108, 20250115],
       "nendo": [2025, 2025, 2025],
       "znen_nendo": [2024,2024,2024],
       "znen_shudo": [51,52,1],
       "minashi_tsuki": [1,2,3]
   })


@patch("google.cloud.storage.Client")
def test_extract_as_df_with_encoding_valid(mock_storage_client):
   # Prepare dummy CSV content
    data = pd.DataFrame({"A": [1,2,3], "B": ["x","y","z"]})
    csv_bytes = make_csv_bytes(data)
    mock_bucket = MagicMock()
    mock_blob = MagicMock()
    mock_blob.download_as_bytes.return_value = csv_bytes
    mock_bucket.blob.return_value = mock_blob
    mock_storage_client.return_value.bucket.return_value = mock_bucket
    # Call your function
    df = common.extract_as_df_with_encoding("dummy.csv", "dummy-bucket", "utf-8")
    assert isinstance(df, pd.DataFrame)
    assert set(df.columns) == set(["A","B"])
    assert len(df) == 3
    
    
# def test_interpolate_df_basic(dummy_sales_df, dummy_calendar_df):
#     result_df = interpolate_df(dummy_sales_df, dummy_calendar_df)
#     assert "URI_SU" in result_df.columns
#     # No NaNs in URI_SU after fillna
#     assert not result_df["URI_SU"].isnull().any()
#     # Interpolated baika_toitsu should no longer contain zeros at places replaced by nan then interpolated
#     assert result_df["baika_toitsu"].dtype == float
#    # Interpolated BAIKA should not have NaNs
#     assert not result_df["BAIKA"].isnull().any()
    
    
def test_calc_nenshudo_increment_decrement():
    max_syudo_dic = {2024: 52, 2025: 52}
   # Test normal increment
    assert calc_nenshudo(202501, 1, max_syudo_dic) == 202502
   # Test increment crossing max boundary
    assert calc_nenshudo(202552, 1, max_syudo_dic) == 202601
   # Test decrement normal
    assert calc_nenshudo(202502, -1, max_syudo_dic) == 202501
   # Test decrement crossing year boundary
    assert calc_nenshudo(202501, -1, max_syudo_dic) == 202452
    
    
    
def test_calc_nenshudo2_offset(monkeypatch):
   # Create dummy DataFrame for nenshudo list
   dfc_tmp = pd.DataFrame({"nenshudo": [202501,202502,202503]})
   # Normal offset
   assert calc_nenshudo2(202501, 2, dfc_tmp) == 202503
   # Offset out of range returns None
   assert calc_nenshudo2(202503, 2, dfc_tmp) is None
   # Nonexistent nenshudo returns None
   assert calc_nenshudo2(999999, 1, dfc_tmp) is None
    
    
@patch("demand_forecast.common.extract_as_df_with_encoding")
def test_load_sales_data_basic(mock_extract):
   # Mock sales csv data
    dummy_data = pd.DataFrame({
       "PRD_CD": [1,2,3],
       "PRICE": [100,200,300]
   })
    mock_extract.return_value = dummy_data
    dpt_list = [1,2]
    bucket_name = "dummy_bucket"
    tenpo_cd = 123
    tenpo_cd_ref = None
    today = "20250101"
    path_tran = "{}/{}_monthly_series.csv"
    df = load_sales_data(dpt_list, tenpo_cd, path_tran, bucket_name, tenpo_cd_ref, today, path_tran)
   # Expect concatenated DataFrame for two departments, so length matches twice dummy_data length
    assert len(df) == 6
   # Columns contain PRD_CD as expected
    assert "PRD_CD" in df.columns
    
    
@patch("demand_forecast.common.extract_as_df_with_encoding")
def test_load_sales_data_new_store_filtering(mock_extract):
    # Mock target store data returns 5 rows with mixed PRD_CD
    sales_data_target = pd.DataFrame({
       "PRD_CD": [1,2,3,4,5],
       "PRICE": [100,200,300,400,500]
    })
    # Mock reference store data returns PRD_CD subset
    sales_data_ref = pd.DataFrame({
       "PRD_CD": [1,3,5],
       "PRICE": [10,20,30]
    })
    def side_effect(path, bucket, encoding="utf-8"):
        if "ref" in path:
            return sales_data_ref
        return sales_data_target
    mock_extract.side_effect = side_effect
    dpt_list = [1]
    bucket_name = "dummy_bucket"
    tenpo_cd = 123
    tenpo_cd_ref = 456
    today = "20250101"
    # path_tran for target store
    path_tran = "{}/{}_monthly_series.csv"
    # path_tran_ref for reference store (inject 'ref' keyword for side_effect)
    path_tran_ref = "{}/ref_{}_monthly_series.csv"
    df = load_sales_data(dpt_list, tenpo_cd, path_tran, bucket_name, tenpo_cd_ref, today, path_tran_ref)
    # Should be filtered to only PRD_CD in reference store [1,3,5]
    assert set(df["PRD_CD"].unique()) == set([1,3,5])
    # TENPO_CD column should be set to tenpo_cd for all rows
    assert (df["TENPO_CD"] == tenpo_cd).all()
    
    
@patch("demand_forecast.common.extract_as_df_with_encoding")
def test_process_minmax_data_basic(mock_extract):
    # Create dummy minmax data with products some with zero min/max and some valid
    minmax_data_1 = pd.DataFrame({
       "PRD_CD": [1, 2, 3, 4],
       "HOJU_MIN_SU": [10, 0, 5, 0],
       "HOJU_MAX_SU": [20, 0, 15, 0],
       "TOROKU_YMD": [20250101]*4,
       "TOROKU_HMS": ["000000"]*4,
       "NENSHUDO": [202501]*4,
    })
    # Mock CSV extract returns minmax_data when called
    mock_extract.return_value = minmax_data_1
    sales_df = pd.DataFrame({
       "PRD_CD": [1, 2, 3, 4, 5],
       "OTHER": [10,11,12,13,14]
    })
    restrict_minmax = True
    dpt_list = [1]
    tenpo_cd = "123"
    bucket_name = "dummy_bucket"
    time_log_list = []
    df_out, no_minmax_df, minmax0_df, _ = process_minmax_data(restrict_minmax, dpt_list, tenpo_cd, bucket_name, sales_df, common.extract_as_df_with_encoding, time_log_list)
    # Products without minmax (5) should appear in no_minmax_df
    assert 5 in no_minmax_df['PRD_CD'].values
    # Products with zero min/max (2 and 4) should appear in minmax0_df
    assert 2 in minmax0_df['PRD_CD'].values
    assert 4 in minmax0_df['PRD_CD'].values
    # Resulting sales_df should only include products with valid minmax (1 and 3)
    assert set(df_out['PRD_CD'].unique()) == {1, 3}
    
    
@patch("demand_forecast.common.extract_as_df")
def test_restrict_tenpo_hacchu_end_func(monkeypatch_extract):
    # Mock order cutoff dataframe
    cutoff_data = pd.DataFrame({
       "TENPO_CD": [123, 123, 123],
       "PRD_CD": [1, 2, 3],
       "HACCHU_TO_YMD": [20240101, 20250101, 20260101]  # Only product 1 and 2 cutoff <= my_date
    })
    monkeypatch_extract.return_value = cutoff_data
    sales_df = pd.DataFrame({
       "PRD_CD": [1, 2, 3, 4],
       "PRICE": [100, 200, 300, 400]
    })
    restrinct_tenpo_hacchu_end = True
    tenpo_cd = 123
    bucket_name = "dummy_bucket"
    my_date = 20250101
    time_log_list = []
    df_filtered, hacchu_prd_df, _ = restrict_tenpo_hacchu_end_func(restrinct_tenpo_hacchu_end, tenpo_cd, bucket_name, sales_df, my_date, common.extract_as_df, time_log_list)
    # Sales df should exclude product 1 and 2 because cutoff date <= my_date
    assert 1 not in df_filtered['PRD_CD'].values
    assert 2 not in df_filtered['PRD_CD'].values
    # Product 3 and 4 should remain
    assert set(df_filtered['PRD_CD']) == {3,4}
    # Check that hacchu_prd_df has correct reason column
    assert all(hacchu_prd_df['reason'] == 'hacchu_teishi')
